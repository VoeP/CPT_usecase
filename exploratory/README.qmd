---
title: "Initial Data Exploration"
output:
  word_document: default
  github_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width=10, 
  fig.height=6,
  eval = T
)
```




### Data dictionary for Cone Penetration Test use case


* **“sondeernummer”**: unique identification number;
* **“(x,y)”**: coordinates in Lambert 72 projection;
* **“Starthoogte sondering”**: height above sea level of the CPT start point;
* **“Einddiepte sondering”**: final depth of the CPT measured relative to surface level;
* **“diepte”**: depth relative to surface level in meters
* **“diepte_mtaw”**: height above sea level in meters
* **“qc”**: measurement of the resistance of the soil to the penetration of the cone tip. Measurement unit is MPa or psi;
* **“fs”**: measurement of the resistance between the soil and the friction sleeve of the CPT tool. Measurement unit is kPa or psi;
* **“Fr”**: friction ratio calculated from qc and fs;
* **“qtn”**: normalized qc accounting for pore water pressure;
* **“Frn”**: normalized friction ratio accounting for pore water pressure;
* **“icn”**: ‘soil behaviour type index’ based on normalized qc and rf;
* **“sbt”**: ‘Soil behaviour Type klasse’ based on icn intervals from the literature. Integer number between 1 and 9;
* **“ksbt”**: estimated hydraulic conductivity derived from icn through empirical correlations;
* **“lithostrat_id”**: lithostratigraphic unit. This is the label we want to determine.



- how the model will handle new soil types it was not trained on
- Continuous learning to adapt to new data
- A time series usually have noise denoise
- goal is to find if we can given
- domain knowledge 
- How to handle rare classes. 
- Distribution 
- Apart from CPT data what type of other data/knowledge  do the geological experts use for soil type classification
- IN the CPT data what signals/features do they look at to be able to know a soil type. How do they know  that a transition occurs at given point , ie do they look at the whole series from soldering id or each soil type has special series features that an expert will know by looking. Do knew layers soil type tend to have low CPT values etc
- How to handle rare classes, What are this soiltype communicating to a geotechnician 
- If a good model is found, do VETO have plans/would want this to have a continuos learning capability. ie by adding labels of pre labelled data.

- Geological domain knowledge from survey data , homogeneous signal/heterogeneous signal. Sharp boundarys vs smooth transitions
- Segmentation, multi instance learning 
- hidden markov model
- [Introduction to Cone Penetration Testing](https://www.ags.org.uk/2022/09/introduction-to-cone-penetration-testing/)
- [Robertson](https://www.cpt-robertson.com/PublicationsPDF/Robertson%20Updated%20SBT%20CGJ%202016.pdf)
- first define soil types using known cone penetration metrics then see which soil lithostart correspond to this then do the prediction


### Belgium-specific CPT/Lithostrat References


* **Rogiers et al. 2017 (PLOS ONE)** — shows **automated lithostrat mapping from CPT SBT** on a large dataset in **northern Belgium**; compares SBT-based modeling vs unsupervised/literature charts and reports efficiency + accuracy gains. ([PLOS](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0176656))
* **Deckers & Goolaerts 2022 (Geologica Belgica)** — **CPT characterization of Middle–Upper Miocene** near **Antwerp Intl. Airport** (Berchem Fm: **Kiel & Antwerpen mbrs**); documents typical qc/Rf expressions and regional correlations. ([popups.uliege.be](https://popups.uliege.be/1374-8505/index.php?id=6999))
* **Schiltz 2020 (Geologica Belgica)** — **NE Belgium (≈60 km²)** case; proposes an **informal stratigraphy from CPT signals** and discusses which CPT-derived patterns distinguish units. ([popups.uliege.be](https://popups.uliege.be/1374-8505/index.php?id=6668&lang=en))
* **DOV Vlaanderen – Sonderingen** — official Flemish **definitions & measurement details** (qc, fs, **Rf**, u₂ optional; spacing, acquisition); aligns your variables with the **DOV data model**. ([Databank Ondergrond Vlaanderen](https://www.dov.vlaanderen.be/page/sonderingen))
* **NCS 2022 Excursion Guide (Neogene of N Belgium)** — regional context; notes **recent improvements in CPT-based correlations** for Antwerp-area units; useful when mapping CPT patterns to named formations. ([Databank Ondergrond Vlaanderen](https://dov.vlaanderen.be/sites/default/files/pfiles_files/NCS2022_ExcursionGuide.pdf))
* **Robertson CPT guides (SBT/qtn–Fr)** — the **standard charts/updates** underpinning SBT interpretation used in Belgian work; keep handy for **qₙ, qc, Rf** mapping. ([CPT Robertson](https://www.cpt-robertson.com/PublicationsPDF/2-56%20RobSBT.pdf))
* **pydov (DOV client) notebooks** — practical examples to **query CPTs by bbox/formation**, fetch profiles, and assemble **median qc/Rf** by depth for template building. ([Pydov](https://pydov.readthedocs.io/en/stable/notebooks/search_sonderingen.html))

- data management plan
- not all information to start implementing
- relation database; tables
- Preprocessing steps, cleaning, filtering, feature engineering: Talk this to the client; works equally good
- minimal viable product (MVP)
- derivative features, interactions, kNN, PCA, tsne, likelihood of layers, then which specific layers
- Sliding window approach, Modelling approach, 80% likelihood of this. Depth, geographical location, spatial correlation, depth profile,
- sequence, temporal patterns,
- multi instance learning, transition period, transient periods
- predict transition periods
- ensemble models, xgboost, lightgbm, catboost
- missing layers - imputation, knn imputation, mice imputation
- learn characteristics of missing layers
- markov models??
- look at the sequence of layer transitions
- geological domain knowledge, homogeneous signal/heterogeneous signal. Sharp boundaries vs smooth transitions
- first maybe group similar layers together, then predict the groups, then within the groups predict the specific layers


```{r}
library(tidyverse)
library(data.table)
library(arrow)
library(here)
library(xgboost)
library(knitr)
main_folder <- "year2/ProjectDataScience/project"
data_folder <- here(main_folder, "data")
cpt_df <- read_parquet(
  paste(
    data_folder,
    "vw_cpt_brussels_params_completeset_20250318_remapped.parquet",
    sep = "/"
  )
)
setDT(cpt_df)
```


- head

```{r}
# filter of na in lithostrat_id
lithostrat_missing_df <- cpt_df[is.na(lithostrat_id)]
cpt_df <- cpt_df[!is.na(lithostrat_id)]
head(cpt_df) %>% kable()
```


```{r}
soldering_id_with_litho_missing <- unique(lithostrat_missing_df$sondering_id)
# without lithostrat_id missing
soldering_id_with_litho <- unique(cpt_df$sondering_id)

id_to_check <- 493
common_soldering_id <- intersect(soldering_id_with_litho_missing, soldering_id_with_litho)
soldering_315_df <- cpt_df[sondering_id == id_to_check]
soldering_315_df_without_litho <- lithostrat_missing_df[sondering_id == id_to_check]
```

```{r}
cpt_df[, .N, by = sondering_id][order(-N)]
cpt_df[, .(unique_drillings = uniqueN(sondering_id))]
```

```{r}
# number of obs per soldering id
cpt_df[, no_obs := .N, by = sondering_id]
obs <- cpt_df[, .(
  min_obs = min(no_obs),
  max_obs = max(no_obs),
  mean_obs = mean(no_obs),
  median_obs = median(no_obs)
)]

kable(obs)
```


- find unique drillings  then map

```{r}
unique_drillings <- cpt_df |>
  unique(by = "sondering_id")

## convert lambert to plot to plot with geom_sf
library(sf)
library(ggplot2)
library(ggspatial)
library(ggrepel)


unique_drillings_sf <- st_as_sf(unique_drillings,
  coords = c("x", "y"), crs = 31370
) |>
  st_transform(crs = 4326)

head(unique_drillings_sf) %>% kable()

# be_lvl2 <- geodata::gadm(
#   country = "BEL",
#   level = 2, path = tempdir()
# ) |>
#   st_as_sf() %>%
#   filter(NAME_2 == "Vlaams Brabant")

# save(be_lvl2, file = paste0(data_folder, "/be_lvl2.rda"))
load(paste0(data_folder, "/be_lvl2.rda"))
ggplot(be_lvl2) +
  geom_sf(fill = "gray") +
  geom_sf(
    data = unique_drillings_sf,
    aes(geometry = geometry),
    color = "blue", size = 0.5
  ) +
  labs(title = "CPT Drillings in Brussels Region") +
  theme_minimal()
```



```{r}

library(zoo)

cpt_df[, rf := (fs / (qc * 1000)) * 100]
window_m <- 0.5  # smoothing window in meters
cpt_df[, dz := median(diff(diepte), na.rm = TRUE), 
       by = sondering_id]
cpt_df[(is.na(dz) | !is.finite(dz) | dz <= 0), dz := 0.02]    # fallback 2 cm
cpt_df[(is.na(dz)), dz := 0.02] 
cpt_df[, k := pmax(5L, as.integer(round(window_m / dz))),
       by = sondering_id]
cpt_df[k %% 2 == 0, k := k + 1L]                 # odd window for centering

cpt_df[!is.na(qc), qc_med := rollmedian(qc, k[1], 
                              fill = "extend", 
                              align = "center"), 
       by = sondering_id]
cpt_df[, rf_med := rollmedian(rf, k[1], 
                              fill = "extend", 
                              align = "center"), 
       by = sondering_id]


cpt_df[, cohesive_flag   := as.integer(rf_med >= 1.5)]
cpt_df[, granular_flag   := as.integer(rf_med <= 1.0)]
cpt_df[, organic_flag    := as.integer(qc_med < 0.5 & rf_med > 5)]


cpt_df[, behavior_basic := fcase(
  qc_med < 0.5 & rf_med > 5,
  "Organic/peat",
  qc_med < 1.5 & rf_med >= 2,
  "Soft clay",
  qc_med < 4.0 & rf_med >= 1.5,
  "Clay/Silty clay",
  between(qc_med, 4.0, 10.0)  &
    rf_med <= 1.5,
  "Sand (loose)",
  between(qc_med, 10.0, 20.0) &
    rf_med <= 1.2,
  "Sand (medium)",
  between(qc_med, 20.0, 40.0) &
    rf_med <= 1.0,
  "Sand (dense)",
  qc_med >= 40.0 & rf_med < 0.5,
  "Gravel/Cemented",
  default = "Mixed/Transitional"
)]

cpt_df[, soft_class := fcase(
  qc_med < 1,
  "Very soft",
  between(qc_med, 1, 2),
  "Soft",
  between(qc_med, 2, 4),
  "Firm",
  between(qc_med, 4, 8),
  "Stiff",
  between(qc_med, 8, 16),
  "Very stiff",
  qc_med >= 16,
  "Hard"
)]

cpt_df[, sand_density := fcase(
  granular_flag == 1 & qc_med < 10,
  "Loose",
  granular_flag == 1 &
    between(qc_med, 10, 20),
  "Medium",
  granular_flag == 1 &
    between(qc_med, 20, 40),
  "Dense",
  granular_flag == 1 & qc_med >= 40,
  "Very dense",
  default = NA_character_
)]

cpt_df[, clay_strength := fcase(
  cohesive_flag == 1 & qc_med < 1,
  "Very soft",
  cohesive_flag == 1 &
    between(qc_med, 1, 2),
  "Soft",
  cohesive_flag == 1 &
    between(qc_med, 2, 4),
  "Firm",
  cohesive_flag == 1 &
    between(qc_med, 4, 8),
  "Stiff",
  cohesive_flag == 1 & qc_med >= 8,
  "Very stiff+",
  default = NA_character_
)]

```

```{r, fig.width=10, fig.height=6}
plot_litho_stack <- function(data,
                             litho_col = "lithostrat_id",
                             class_col = "behavior_basic",
                             title = NULL,
                             palette = NULL) {
  stopifnot(all(c(litho_col, class_col) %in% names(data)))

  df <- data[, .N, by = c(litho_col, class_col)]
  df[, prop := N / sum(N), by = litho_col]

  if (is.null(title)) {
    title <- sprintf("Share of %s classes within each %s",
                     class_col, litho_col)
  }

  ggplot(df, aes(x = .data[[litho_col]], y = prop, fill = .data[[class_col]])) +
    geom_col(position = "stack", width = 0.9) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_manual(values = palette %||% scales::hue_pal()(n_distinct(df[[class_col]]))) +
    labs(
      x = "Lithostratigraphic unit",
      y = "Share (%)",
      fill = class_col,
      title = title
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 35, hjust = 1))
}

plot_litho_stack(cpt_df, class_col = "behavior_basic")

```


```{r, fig.width=10, fig.height=6}
plot_litho_stack(cpt_df, class_col = "soft_class")
```







### What we learned from the literature (CPT → soil subdivision)

* From the literature, we learned that **soil layers can be divided using CPT-based soil properties** such as cone resistance (**qc**), sleeve friction (**fs**), friction ratio (**Rf = fs/qc**), and normalized indices (**Qtn**, **Fr**).
* **Standard CPT charts** (e.g., Robertson) link these properties to **soil behavior types (SBT)** like clay, silt, sand, and dense sand/gravel.
* Several Belgian studies have shown how CPT data can be used to **map lithostratigraphic units more efficiently**:


### Figure: CPT Robertson plots

* The figure below is **derived from CPT measurements**, guided by **methods described in the literature**.
* It shows how **different soil types can be distinguished** using measured and normalized CPT properties:

  * Left: **qc–Rf space**  shows the relation between cone resistance and friction ratio.
  * Right: **Qtn–Fr space** shows normalized values useful for comparing different sites.
* Colors represent **soil behavior groups** such as *silt/sandy silt*, *sand*, *mixed/transitional*, and *dense sand/gravel*.




```{r, fig.width=10, fig.height=6}

library(scales)
library(patchwork)
library(hexbin)


cpt_df[, rf_ratio := fs / qc]
cpt_df[, rf_pct   := 100 * rf_ratio]

# 'fr' appears to be percent already but can have big outliers.
# Make an explicit 'fr_pct' and lightly winsorize for visuals/features.
cpt_df[, fr_pct := as.numeric(fr)]
cpt_df[is.finite(fr_pct), fr_pct := pmin(fr_pct, 20)]   


# summary(cpt_df[, .(qc, fs, rf_ratio, rf_pct, qtn, fr_pct)])


# Prefer normalized Qtn–Fr space if both present; otherwise fall back to qc–Rf.
has_norm <- all(c("qtn", "fr_pct") %in% names(cpt_df))

if (has_norm) {
  # thresholds are sensible starters; tune per basin if desired
  cpt_df[, behavior := fcase(
    fr_pct >= 2   & qtn < 10,                     "Clay / organic",
    fr_pct >= 1   & qtn >= 10 & qtn < 50,         "Silt / sandy silt",
    fr_pct <  1.5 & qtn >= 50 & qtn < 200,        "Sand",
    fr_pct <  0.5 & qtn >= 200,                   "Dense sand / gravel",
    default = "Mixed / transitional"
  )]
} else {
  cpt_df[, behavior := fcase(
    qc < 0.5  & rf_pct > 5,                       "Organic / peat",
    qc < 1.5  & rf_pct >= 2,                       "Clay (soft)",
    qc < 4.0  & rf_pct >= 1.5,                     "Clay / silty clay",
    qc >= 4.0  & qc < 10  & rf_pct <= 1.5,         "Sand (loose)",
    qc >= 10.0 & qc < 20  & rf_pct <= 1.2,         "Sand (medium)",
    qc >= 20.0 & qc < 40  & rf_pct <= 1.0,         "Sand (dense)",
    qc >= 40.0 & rf_pct <  0.5,                    "Dense sand / gravel",
    default = "Mixed / transitional"
  )]
}

cpt_df[, behavior := factor(behavior)]


```


```{r}
plot_robertson_pair <- function(dt,
                                sonder_id,
                                color_var = "behavior",
                                palette = "Set2",
                                point_alpha = 0.55,
                                point_size = 1,
                                title_prefix = "CPT Robertson plots") {
  stopifnot(data.table::is.data.table(dt))
  stopifnot(color_var %in% names(dt))

  sub_dt <- dt[sondering_id == sonder_id]
  if (nrow(sub_dt) == 0L) stop("No rows for sondering_id = ", sonder_id)

  col_sym <- rlang::ensym(color_var)

  pA <- ggplot(sub_dt, aes(x = rf_pct, y = qc, color = !!col_sym)) +
    geom_point(alpha = point_alpha, size = point_size) +
    scale_y_log10(name = expression(paste("Cone resistance  ", q[c], " (MPa)")),
                  breaks = c(0.1, 0.5, 1, 5, 10, 50),
                  limits = c(0.1, 60)) +
    scale_x_continuous(name = expression(paste("Friction ratio  ", R[f], " (%)")),
                       limits = c(0, 8),
                       expand = expansion(mult = c(0.01, 0.05))) +
    labs(title = "qc–Rf space", color = color_var) +
    theme_minimal(base_size = 11)

  pB <- ggplot(sub_dt, aes(x = fr_pct, y = qtn, color = !!col_sym)) +
    geom_point(alpha = point_alpha, size = point_size) +
    scale_x_log10(name = expression(paste("Normalized friction ratio  ", F[r], " (%)")),
                  breaks = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
                  limits = c(0.1, 10)) +
    scale_y_log10(name = expression(paste("Normalized cone resistance  ", Q[t][n])),
                  breaks = c(1, 5, 10, 50, 100, 500),
                  limits = c(1, 600)) +
    labs(title = "Qtn–Fr space", color = color_var) +
    theme_minimal(base_size = 11)

  if (!is.null(palette)) {
    pA <- pA + scale_color_brewer(palette = palette)
    pB <- pB + scale_color_brewer(palette = palette)
  }

  (pA | pB) +
    patchwork::plot_annotation(
      title = sprintf("%s — sondering %s", title_prefix, sonder_id)
    )
}


ids <- sample(unique(cpt_df$sondering_id), 10)
for (sid in ids) print(plot_robertson_pair(cpt_df, sid, color_var = "behavior"))

```


- Color the same abobe with  lithorstat ID

```{r}
for (sid in ids) print(plot_robertson_pair(cpt_df, sid, color_var = "lithostrat_id"))
```


- Randomly select 10 drillings and plot cpt data

```{r, fig.width=10, fig.height=6}
set.seed(124)
sampled_drillings <- sample(unique(cpt_df$sondering_id), 6)
sampled_data <- cpt_df[sondering_id %in% sampled_drillings]
data.table::setorder(sampled_data, index)



plot_cpt_series <- function(data,
                            depth_col = "diepte",
                            value_col = "qc",
                            color_var = "lithostrat_id",
                            group_var = NULL,
                            facet_var = "sondering_id",
                            title = NULL,
                            depth_label = "Depth below surface (m)",
                            value_label = NULL,
                            ncol_facet = 3,
                            log_value = FALSE,
                            reverse_depth = TRUE,
                            flip_coords = TRUE,
                            alpha_line = 0.7,
                            alpha_point = 0.6,
                            point_size = 0.6,
                            legend_position = "bottom") {
  depth_sym <- rlang::ensym(depth_col)
  value_sym <- rlang::ensym(value_col)
  color_sym <- if (is.null(color_var)) {
    NULL
  } else {
    rlang::ensym(color_var)
  }
  group_sym <- if (is.null(group_var) && !is.null(color_var)) {
    rlang::ensym(color_var)
  } else if (is.null(group_var)) {
    rlang::ensym(facet_var)
  } else {
    rlang::ensym(group_var)
  }
  value_label <- value_label %||% value_col

  p <- ggplot(data, aes(x = !!depth_sym, y = !!value_sym)) +
    geom_line(aes(group = interaction(!!rlang::enquo(group_sym))), alpha = alpha_line) +
    geom_point(aes(group = interaction(!!rlang::enquo(group_sym))), size = point_size, alpha = alpha_point)

  if (!is.null(color_sym)) {
    p <- p + aes(color = factor(!!color_sym))
  }

  if (log_value) {
    p <- p + scale_y_log10()
  }
  if (reverse_depth) {
    p <- p + scale_x_reverse()
  }
  if (flip_coords) {
    p <- p + coord_flip()
  }

  p +
    facet_wrap(stats::as.formula(paste("~", facet_var)), ncol = ncol_facet) +
    labs(
      title = title %||% sprintf("CPT Profiles (%s vs depth)", value_col),
      x = depth_label,
      y = value_label,
      color = color_var %||% NULL
    ) +
    theme_minimal() +
    theme(legend.position = legend_position)
}


plot_cpt_series(sampled_data,
  value_col = "qc",
  title = "Cone resistance"
)
```

```{r}
plot_cpt_series(sampled_data,
  value_col = "fr",
  title = "friction"
)
```

- lets smooth the qc and fs with rolling mean of window 3
- is the frequency obeying some symmetry

```{r}
# use decompose function to
extract_trend <- function(x, freq = 5) {
  decomposed <- decompose(ts(x, frequency = freq))
  return(decomposed$trend)
}

freq_from_depth <- function(z, default = 25L) {
  if (length(z) < 2L) {
    return(default)
  }
  dz <- stats::median(diff(sort(z)), na.rm = TRUE)
  if (!is.finite(dz) || dz <= 0) {
    return(default)
  }
  as.integer(max(1, round(1 / dz)))
}

freq_from_depth <- function(z, default = 25L) {
  if (length(z) < 2L) {
    return(default)
  }
  dz <- stats::median(diff(sort(z)), na.rm = TRUE)
  if (!is.finite(dz) || dz <= 0) {
    return(default)
  }
  as.integer(max(1, round(1 / dz)))
}


sampled_data[, fr_smooth := extract_trend(fr, freq = freq_from_depth(diepte)),
  by = sondering_id
]
sampled_data[, qc_smooth := extract_trend(qc, freq = freq_from_depth(diepte)),
  by = sondering_id
]
```


- plot qc_smooth

```{r}
plot_cpt_series(sampled_data,
  value_col = "qc_smooth",
  title = "Smoothed Cone resistance (trend)"
)
```

- plot fr_smooth

```{r}
plot_cpt_series(sampled_data,
  value_col = "fr_smooth",
  title = "Smoothed friction (trend)"
)
```


```{r}
library(signal)
library(pracma)
# Zero-phase Butterworth low-pass
butter_zero_phase <- function(x, dz, fc = 1.0, order = 4L) {
  # fc in cycles/m; dz in meters/sample
  if (!is.finite(dz) || dz <= 0 || all(!is.finite(x))) return(x)
  nyq <- 1/(2*dz)                    # Nyquist (cycles/m)
  Wc  <- min(0.99, max(1e-6, fc/nyq))# normalized cutoff 0..1
  bf  <- signal::butter(order, Wc, type = "low")
  # detrend first to keep DC drift from dominating:
  xd  <- pracma::detrend(x, tt = "linear")
  as.numeric(signal::filtfilt(bf, xd))
}

vars_to_plot <- c("qc", "fs", "rf_pct", "qtn", "fr_pct", "fr")

filterd_vars <- paste0(vars_to_plot, "butter_zero")
cpt_df[, dz := median(diff(diepte), na.rm = TRUE), by = sondering_id]
cpt_df[, (filterd_vars) := lapply(.SD, butter_zero_phase, dz =dz[1]),
       .SDcols = vars_to_plot,
       by = sondering_id
]

```

```{r}

```
 
 
```{r}
# boxplotqc, rf_pct, qtn, fr by lithostrat_id

vars_to_plot <- c("qc", "fs", "rf_pct", "qtn", "fr_pct", "fr")

lapply(X = vars_to_plot, FUN = function(var) {
    p <- ggplot(cpt_df, aes(x = lithostrat_id, y = log(.data[[var]]), fill = lithostrat_id)) +
        geom_boxplot() +
        scale_y_log10() +
        labs(
        title = paste("Distribution of", var, "Values by Lithostrat Unit"),
        x = "Lithostratigraphic Unit",
        y = paste(var, "[log scale]"),
        fill = "Lithostrat unit"
        ) +
        theme_minimal() +
        theme(
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none"
        )
    p
    
    })


```



```{r qc_summaries_simple, fig.width=12, fig.height=6}
cpt_model_df <- cpt_df[!is.na(qc) & !is.na(lithostrat_id) & !is.na(fs)]

# 1) Five-number summary of qc PER DRILLING (sondering_id)

# cut depth into bins of 1m
cpt_model_df[, diepte := round(diepte)]
cpt_model_df[, diepte_bin := cut(diepte, breaks = seq(
  from = 0, to = max(diepte, na.rm = TRUE), by = 5
))]

summ_by_sonder <- cpt_model_df[
  , .(
    n          = .N,
    depth_min  = min(diepte, na.rm = TRUE),
    depth_max  = max(diepte, na.rm = TRUE),
    qc_min     = min(qc, na.rm = TRUE),
    qc_q25     = quantile(qc, 0.25, na.rm = TRUE),
    qc_med     = median(qc, na.rm = TRUE),
    qc_q75     = quantile(qc, 0.75, na.rm = TRUE),
    qc_max     = max(qc, na.rm = TRUE),
    qc_mean    = mean(qc, na.rm = TRUE),
    qc_sd      = sd(qc, na.rm = TRUE)
  ),
  by = .(sondering_id, diepte_bin, lithostrat_id)
]

library(knitr)
kable(head(summ_by_sonder, 10))
summ_by_sonder_melt <- melt(
  summ_by_sonder,
  id.vars = c("sondering_id", "diepte_bin", "lithostrat_id"),
  variable.name = "statistic",
  value.name = "value",
  variable.factor = FALSE
)
# remove N

summ_by_sonder_melt <- summ_by_sonder_melt[statistic != "n"]

head(summ_by_sonder_melt)
## boxplot of qc per lithostrat_id
# ggplot(summ_by_sonder_melt, aes(x = lithostrat_id, y = value, fill = as.factor(diepte))) +
#   geom_boxplot() +
#   scale_y_log10() +
#   labs(
#     title = "Distribution of Cone Resistance (qc) Values",
#     x = "Lithostratigraphic Unit",
#     y = "Cone Resistance (qc) [MPa, log scale]"
#   ) +
#   theme_minimal() +
#   theme(legend.position = "none") +
#   facet_wrap(~ statistic, ncol = 2)

stats_vars <- unique(summ_by_sonder_melt$statistic) %>% sample()
# stats_vars[1:3]
# find unique depth
unique_depths <- unique(summ_by_sonder_melt$diepte)
length(unique_depths)
for (s in stats_vars) {
  df_s <- summ_by_sonder_melt[statistic == s]

  p <- ggplot(
    df_s,
    aes(x = lithostrat_id, y = value, fill = as.factor(diepte_bin))
  ) +
    geom_boxplot() +
    scale_y_log10() +
    labs(
      title = paste("Distribution of qc by Lithostrat (statistic:", s, ")"),
      x = "Lithostratigraphic Unit",
      y = "Cone Resistance (qc) [MPa, log scale]",
      fill = "Depth bin (m)"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )

  print(p)
}
# ..
```


### Scatter of depth vs qc colored by lithostrat_id

```{r , fig.width=10, fig.height=6}
ggplot(cpt_model_df, aes(
  x = diepte,
  y = qc,
  color = as.factor(lithostrat_id)
)) +
  geom_point(alpha = 0.5, size = 0.7) +
  scale_y_log10() +
  scale_x_reverse() +
  coord_flip() +
  labs(
    title = "Scatter Plot of Cone Resistance (qc) vs Depth",
    x = "Depth below surface (m)",
    y = "Cone Resistance (qc) [MPa, log scale]",
    color = "Lithostrat unit"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


```{r , fig.width=10, fig.height=6}
ggplot(cpt_model_df, aes(
  x = diepte,
  y = fr,
  color = as.factor(lithostrat_id)
)) +
  geom_point(alpha = 0.5, size = 0.7) +
  scale_y_log10() +
  scale_x_reverse() +
  coord_flip() +
  labs(
    title = "",
    x = "Depth below surface (m)",
    y = "Cone Resistance (qc) [MPa, log scale]",
    color = "Lithostrat unit"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```




```{r , fig.width=10, fig.height=6}
ggplot(cpt_model_df, aes(
  x = diepte,
  y = fs,
  color = as.factor(lithostrat_id)
)) +
  geom_point(alpha = 0.5, size = 0.7) +
  scale_y_log10() +
  scale_x_reverse() +
  coord_flip() +
  labs(
    title = "Scatter Plot of Cone Resistance (fs) vs Depth",
    x = "Depth below surface (m)",
    y = "Cone Resistance (qc) [MPa, log scale]",
    color = "Lithostrat unit"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```



- bar plot of lithostrat_id  vs depth bins

```{r litho_depth_bar, fig.width=10, fig.height=6}
# bar plot of lithostrat_id  vs depth bins
ggplot(cpt_model_df, aes(x = diepte_bin, fill = as.factor(lithostrat_id))) +
  geom_bar(position = "fill") +
  labs(
    title = "Proportion of Lithostrat Units by Depth Bins",
    x = "Depth Bins (m)",
    y = "Proportion",
    fill = "Lithostrat unit"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )
```


```{r}
cpt_seq <- cpt_df[, .(col_seq = paste0(rev(unique(lithostrat_id)), 
                                       collapse = "|"),
                      len = uniqueN(lithostrat_id)), 
                  by = sondering_id]

# depth by soldering id
soldering_depth <- cpt_df[, .(min_depth = min(diepte, na.rm = TRUE),
                           max_depth = max(diepte, na.rm = TRUE)),
                       by = sondering_id]
soldering_depth[, depth_range := max_depth - min_depth]

# merge
cpt_seq <- merge(cpt_seq, 
                 soldering_depth, 
                 by = "sondering_id", 
                 all.x = TRUE)
# filter off where len > 1
cpt_seq <- cpt_seq[len > 1]
head(cpt_seq, 20) %>% kable()
```

```{r}
cpt_seq[, layers := strsplit(col_seq, "\\|")]

# Extract all consecutive pairs per sondering
pair_dt <- cpt_seq[, {
  lyr <- unlist(layers)
  if (length(lyr) < 2) {
    data.table(from_layer = character(), to_layer = character())
  } else {
    data.table(
      from_layer = lyr[-length(lyr)],
      to_layer   = lyr[-1]
    )
  }
}, by = sondering_id]

pair_dt[, two_layers := paste(from_layer, to_layer, sep = " -> ")]
kable(pair_dt[1:10])
```

```{r}
#melt to get individual lirthostrat_id

pair_dt_melt <- melt(pair_dt,
                     id.vars = c("sondering_id", "two_layers"),
                     measure.vars = c("from_layer", "to_layer"),
                     variable.name = "layer_type",
                     value.name = "lithostrat_id")
```




```{r}
trimmed_mean <- function(x, trim = 0.1, ...) {
  mean(x, trim = trim, ...)
}

dcast_cpt_wide <- function(dt,
                           value_cols,
                           depth_bin = "diepte_bin",
                           id_cols = c("sondering_id", "lithostrat_id"),
                           agg_fun = trimmed_mean,
                           fill_value = NA,
                           na_rm_cols = TRUE) {
  stopifnot(all(value_cols %in% names(dt)))
  wide_list <- lapply(value_cols, function(vcol) {
    myformula <- as.formula(paste(
      paste(id_cols,
        collapse = "+"
      ),
      "~", depth_bin
    ))

    out <- data.table::dcast(
      myformula,
      value.var = vcol,
      fun.aggregate = agg_fun,
      na.rm = TRUE,
      data = dt,
      fill = fill_value
    )
    setnames(
      out, setdiff(names(out), id_cols),
      paste0(setdiff(
        names(out),
        id_cols
      ), "_", vcol)
    )
    out
  })
  wide_dt <- Reduce(function(a, b) {
    merge(a, b, by = id_cols, all = TRUE)
  }, wide_list)
  if (na_rm_cols) {
    keep <- colSums(is.na(wide_dt)) < nrow(wide_dt)
    wide_dt <- wide_dt[, ..keep]
  }
  wide_dt[]
}

# bins of one meter
cpt_model_df <- cpt_model_df[!is.na(diepte)]
cpt_model_df[, diepte_bin := cut(diepte,
  breaks = seq(
    from = 0, to = max(diepte, na.rm = TRUE) + 5, by = 1
  ),
  include.lowest = TRUE
)]


# cpt_model_df[!is.na(fs), fs_smooth := extract_trend(fs, freq = freq_from_depth(diepte)),
#              by = sondering_id]
# cpt_model_df[!is.na(qc), qc_smooth := extract_trend(qc, freq = freq_from_depth(diepte)),
#              by = sondering_id]


value_vars <- c("rf_ratio", "qc", "qtn") # fs", "fr", "icn")

value_vars <- c("rf_pctbutter_zero", "qcbutter_zero", "qtnbutter_zero", "frbutter_zero") 
cpt_model_df1  <- merge(cpt_model_df,
                       pair_dt_melt, 
                       by = c("sondering_id", "lithostrat_id"),
                       all.y = TRUE,
                       allow.cartesian = TRUE)

setorder(cpt_model_df1, sondering_id, -diepte)
cpt_wide <- dcast_cpt_wide(cpt_model_df,
  value_cols = value_vars,
  id_cols = c("sondering_id", "lithostrat_id"),
  fill_value = 0
)
cpt_wide <- cpt_wide[sondering_id %in% pair_dt$sondering_id]
```


```{r}


widen_factor_indicators <- function(dt,
                                    factor_cols,
                                    id_cols = c("sondering_id", "lithostrat_id"),
                                    na_label = NULL) {
  stopifnot(data.table::is.data.table(dt))
  stopifnot(all(c(factor_cols, id_cols) %in% names(dt)))

  widen_one <- function(col) {
    tmp <- dt[, c(id_cols, col), with = FALSE]
    setnames(tmp, col, "factor_value")
    if (!is.null(na_label)) tmp[is.na(factor_value), factor_value := na_label]

    wide <- dcast(
      tmp,
      as.formula(paste(paste(id_cols, collapse = " + "), "~ factor_value")),
      value.var = "factor_value",
      fun.aggregate = function(x) as.integer(any(!is.na(x))),
      fill = 0
    )

    value_cols <- setdiff(names(wide), id_cols)
    setnames(wide, value_cols, paste0(col, "_", value_cols))
    wide
  }

  merged <- Reduce(function(a, b) merge(a, b, by = id_cols, all = TRUE),
                   lapply(factor_cols, widen_one))
  num_cols <- setdiff(names(merged), id_cols)
  merged[, (num_cols) := lapply(.SD, function(v) fifelse(is.na(v), 0L, v)), .SDcols = num_cols]
  merged[]
}



factor_vars <- c("behavior", "soft_class")#"soft_class"
cpt_wide_factors <- widen_factor_indicators(cpt_model_df, factor_vars)

cpt_wide <- merge(cpt_wide, cpt_wide_factors,
  by = c("sondering_id", "lithostrat_id"),
  all.x = TRUE
)
```



```{r}
ggplot(cpt_wide, aes(x = lithostrat_id, fill = as.factor(lithostrat_id))) +
  geom_bar() +
  labs(
    title = "Bar Plot of Lithostratigraphic Units",
    x = "Lithostratigraphic Unit",
    y = "Count",
    fill = "Lithostratigraphic Unit"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```




```{r, eval=FALSE}
compress_to_other <- function(dt, col, min_count = 10, new_col = NULL) {
  stopifnot(data.table::is.data.table(dt))
  if (is.null(new_col)) new_col <- paste0(col, "_group")

  counts <- dt[, .N, by = col]
  rare_values <- counts[N < min_count][[col]]

  dt[, (new_col) := get(col)]
  if (length(rare_values)) {
    dt[get(col) %in% rare_values, (new_col) := "Other"]
  }
  dt[, (new_col) := factor(get(new_col))]
  invisible(dt)
}


compress_to_other(cpt_wide,
  col = "lithostrat_id",
  min_count = 10,
  new_col = "lithostrat_id_group"
)

ggplot(cpt_wide, aes(x = lithostrat_id_group, fill = as.factor(lithostrat_id_group))) +
  geom_bar() +
  labs(
    title = "Bar Plot of Lithostratigraphic Units",
    x = "Lithostratigraphic Unit",
    y = "Count",
    fill = "Lithostratigraphic Unit"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```


```{r}

library(xgboost)
library(rsample) 
library(yardstick)

cpt_wide[, lithostrat_id_group := lithostrat_id ]

# 0) Keep only rows with a target
cpt_wide <- cpt_wide[!is.na(lithostrat_id_group)]
#save(cpt_wide, file = here(data_folder, "cpt_wide.rda"))
# 1) Define features (drop ids/target)
feat_cols <- setdiff(names(cpt_wide),
                     c("sondering_id", "lithostrat_id", "lithostrat_id_group", "two_layers"))
#  numeric matrix
X <- as.matrix(cpt_wide[, ..feat_cols])
storage.mode(X) <- "double"

# 2) Encode labels 0..K-1 for xgboost
cpt_wide[, y := as.integer(factor(lithostrat_id_group)) - 1L]
label_levels <- levels(factor(cpt_wide$lithostrat_id_group))
y <- cpt_wide$y
num_class <- length(unique(y))

# 3) Grouped 5-fold CV by sondering_id (sondering_id NOT in the model)
set.seed(42)
gfolds <- group_vfold_cv(
  data = data.frame(id = seq_len(nrow(cpt_wide)),
                    sondering_id = cpt_wide$sondering_id),
  group = sondering_id,
  v = 5
)

oof_prob <- matrix(NA_real_, nrow = nrow(X), ncol = num_class)

# 4) XGBoost params (sensible defaults; tune later)
params <- list(
  objective = "multi:softprob",
  num_class = num_class,
  max_depth = 5,
  eta = 0.05,
  subsample = 0.9,
  colsample_bytree = 0.8,
  eval_metric = "mlogloss",
  tree_method = "hist"
)

for (sp in gfolds$splits) {
  va_idx <- assessment(sp)$id
  tr_idx <- analysis(sp)$id
  dtr <- xgb.DMatrix(X[tr_idx, , drop = FALSE], label = y[tr_idx])
  dva <- xgb.DMatrix(X[va_idx, , drop = FALSE], label = y[va_idx])
  bst <- xgb.train(
    params = params,
    data = dtr,
    nrounds = 400,
    verbose = 0
  )
  oof_prob[va_idx, ] <- matrix(predict(bst, dva), ncol = num_class, byrow = TRUE)
}

# 5) OOF evaluation
oof_pred <- max.col(oof_prob) - 1L
eval_dt <- data.table(
  truth = factor(y, levels = 0:(num_class - 1), labels = label_levels),
  pred  = factor(oof_pred, levels = 0:(num_class - 1), labels = label_levels)
)

metric_set <- yardstick::metric_set(accuracy,
                                    kap, f_meas, 
                                    recall, 
                                    precision)

kable(metric_set(eval_dt,
                 truth = truth, 
                 estimate = pred))



```





```{r}
# Confusion matrix:
mt = conf_mat(eval_dt, 
              truth = truth, 
              estimate = pred)

mt[[1]] |> kable()

```


```{r}
bst_full <- xgb.train(
  params = params,
  data = xgb.DMatrix(X, label = y),
  nrounds = 400,
  verbose = 0
)

kable(xgb.importance(model = bst_full,
                     feature_names = feat_cols))

```


```{r}

setDT(eval_dt)

cm_counts <- dcast(eval_dt[, .N, 
                           by = .(truth, pred)], 
                   truth ~ pred, value.var = "N", 
                   fill = 0)
kable(cm_counts)



```



```{r}

pred_cols <- setdiff(names(cm_counts), "truth")
cm_counts[, row_total := rowSums(.SD), .SDcols = pred_cols]
cm_props <- copy(cm_counts)
cm_props[, row_total := rowSums(.SD), .SDcols = pred_cols]
cm_props[, (pred_cols) := lapply(.SD, function(x)
  ifelse(row_total > 0, x / row_total, NA_real_)), .SDcols = pred_cols]

kable(cm_props)

```

```{r}
# melt by truth and row totals then filter
cmprops_melt <- melt(cm_props,
  id.vars = c("truth", "row_total"),
  variable.name = "pred",
  value.name = "prop"
)

# melt cm_counts by truth and row totals then ,erge with cmprops_melt
cmcounts_melt <- melt(cm_counts,
  id.vars = c("truth", "row_total"),
  variable.name = "pred",
  value.name = "count"
)

cm_melt <- merge(cmprops_melt, cmcounts_melt,
  by = c("truth", "pred", "row_total"),
  all.x = TRUE
)
# prop != 0
cm_melt <- cm_melt[!is.na(prop) & prop > 0]
kable(cm_melt)
```




```{r}

library(tidymodels)
library(workflowsets)
library(doParallel)

set.seed(42)

# 
# Ensure outcome is a factor

# remove soil type with less than 5 samples

litho_n <- cpt_wide[, .N, by = lithostrat_id]
rare_litho <- litho_n[N < 5, lithostrat_id]
cpt_wide <- cpt_wide[!lithostrat_id %in% rare_litho]
cpt_wide <- cpt_wide[, lithostrat_id := as.factor(lithostrat_id)]

# 
id_cols <- c(
             "lithostrat_id", "lithostrat_id_group",
             "two_layers", "y")             

# come up with x cols

x_cols <- setdiff(names(cpt_wide),id_cols)

## add back ticks 
x_cols_quoted <- paste0("`", x_cols, "`")
formula <- paste("lithostrat_id ~", 
    paste(x_cols_quoted, collapse = " + ")) %>%
  as.formula()


# Grouped CV (no stratification in group_vfold_cv) 

gfolds <- rsample::group_vfold_cv(cpt_wide, 
                                  group = sondering_id, 
                                  v = 5)




# Trees don't need scaling, but step_zv helps remove single-valued cols
rec <- recipe(formula, data = cpt_wide) |>
  update_role(sondering_id, new_role = "group_id") |>
  step_rm(sondering_id) |>
  step_zv()

# Model specs
rf_spec <- rand_forest(
  mtry  = tune(),
  min_n = tune(),
  trees = 1000
) |>
  set_mode("classification") |>
  set_engine("ranger", importance = "impurity", probability = TRUE)

xgb_spec <- boost_tree(
  trees          = 1000,
  tree_depth     = tune(),
  learn_rate     = tune(),
  loss_reduction = tune(),
  min_n          = tune(),
  mtry           = tune(),
  sample_size    = tune()
) |>
  set_mode("classification") |>
  set_engine("xgboost")

# Build workflows FIRST (required to extract parameter sets)
wf_rf  <- workflow() |> add_recipe(rec) |> add_model(rf_spec)
wf_xgb <- workflow() |> add_recipe(rec) |> add_model(xgb_spec)

# Extract tunable parameters from workflows
rf_params  <- extract_parameter_set_dials(wf_rf)
xgb_params <- extract_parameter_set_dials(wf_xgb)

# Finalize unknown parameters (mtry, sample_size) using the training data
rf_params  <- finalize(rf_params,  cpt_wide[, ..x_cols])
xgb_params <- finalize(xgb_params, cpt_wide[, ..x_cols])

# Generate grids
rf_grid  <- grid_latin_hypercube(rf_params,  size = 20)
xgb_grid <- grid_latin_hypercube(xgb_params, size = 30)

# Workflow set (for parallel tuning)
wfs <- workflow_set(
  preproc = list(base = rec),
  models  = list(rf = rf_spec, xgb = xgb_spec)
)

multi_metrics <- yardstick::metric_set(
  accuracy, 
  bal_accuracy,
  kap)


ctrl <- control_grid(
  save_pred = TRUE, save_workflow = TRUE,
  parallel_over = "resamples", allow_par = TRUE, verbose = TRUE
)

library(future)
library(furrr)

# Set up parallel backend (leave 4 cores free)
plan(multisession, workers = parallel::detectCores() - 4)
# Tune both models on the same resamples ---
# Supply per-model grids using a named list that matches wfs$model labels
# Tune RF
rf_res <- tune_grid(
  wf_rf,
  resamples = gfolds,
  grid      = rf_grid,
  metrics   = multi_metrics,
  control   = ctrl
)

# Tune XGBoost
xgb_res <- tune_grid(
  wf_xgb,
  resamples = gfolds,
  grid      = xgb_grid,
  metrics   = multi_metrics,
  control   = ctrl
)

plan(sequential) # back to sequential
# Best configs
best_rf  <- select_best(rf_res,  metric = "accuracy")
best_xgb <- select_best(xgb_res, metric = "accuracy")

# Finalize workflows
wf_rf_final  <- finalize_workflow(wf_rf,  best_rf)
wf_xgb_final <- finalize_workflow(wf_xgb, best_xgb)

# Refit for OOF predictions
ctrl_oof <- control_resamples(save_pred = TRUE)

rf_oof  <- fit_resamples(wf_rf_final,  resamples = gfolds, metrics = multi_metrics, control = ctrl_oof)
xgb_oof <- fit_resamples(wf_xgb_final, resamples = gfolds, metrics = multi_metrics, control = ctrl_oof)

# Collect metrics
rf_metrics  <- collect_metrics(rf_oof)  |> mutate(model = "ranger")
xgb_metrics <- collect_metrics(xgb_oof) |> mutate(model = "xgboost")

oof_metrics <- bind_rows(rf_metrics, xgb_metrics) |> arrange(.metric, desc(mean))

# Confusion matrices
rf_preds  <- collect_predictions(rf_oof)
xgb_preds <- collect_predictions(xgb_oof)

rf_cm  <- conf_mat(rf_preds,  truth = lithostrat_id, estimate = .pred_class)
xgb_cm <- conf_mat(xgb_preds, truth = lithostrat_id, estimate = .pred_class)





```

```{r}

oof_metrics

```


```{r}
rf_cm 
```


```{r}
xgb_cm
```

```{r}
#
oof_wide <- oof_metrics |>
  dplyr::select(model, .metric, mean, n, std_err) |>
  tidyr::pivot_wider(names_from = model, values_from = c(mean, std_err, n))

kable(oof_wide)
```

