---
title: "Initial Data Exploration"
output:
  word_document: default
  github_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width=10, 
  fig.height=6,
  eval = T
)
```


### Project Folder google drive

- [Project Data Science - Year 2 - UHasselt MSc - Google Drive](https://drive.google.com/drive/folders/1zF4BmxsynoA-PJ4ole8YZemfu0R3WHpG?usp=drive_link)

### Data dictionary for Cone Penetration Test use case

- Quartair soils in the Brussels region

* **“sondeernummer”**: unique identification number;
* **“(x,y)”**: coordinates in Lambert 72 projection;
* **“Starthoogte sondering”**: height above sea level of the CPT start point;
* **“Einddiepte sondering”**: final depth of the CPT measured relative to surface level;
* **“diepte”**: depth relative to surface level in meters
* **“diepte_mtaw”**: height above sea level in meters
* **“qc”**: measurement of the resistance of the soil to the penetration of the cone tip. Measurement unit is MPa or psi;
* **“fs”**: measurement of the resistance between the soil and the friction sleeve of the CPT tool. Measurement unit is kPa or psi;
* **“Fr”**: friction ratio calculated from qc and fs;
* **“qtn”**: normalized qc accounting for pore water pressure;
* **“Frn”**: normalized friction ratio accounting for pore water pressure;
* **“icn”**: ‘soil behaviour type index’ based on normalized qc and rf;
* **“sbt”**: ‘Soil behaviour Type klasse’ based on icn intervals from the literature. Integer number between 1 and 9;
* **“ksbt”**: estimated hydraulic conductivity derived from icn through empirical correlations;
* **“lithostrat_id”**: lithostratigraphic unit. This is the label we want to determine.



- how the model will handle new soil types it was not trained on
- Continuous learning to adapt to new data
- A time series usually have noise denoise
- goal is to find if we can given
- domain knowledge 
- How to handle rare classes. 
- Distribution 
- Apart from CPT data what type of other data/knowledge  do the geological experts use for soil type classification
- IN the CPT data what signals/features do they look at to be able to know a soil type. How do they know  that a transition occurs at given point , ie do they look at the whole series from soldering id or each soil type has special series features that an expert will know by looking. Do knew layers soil type tend to have low CPT values etc
- How to handle rare classes, What are this soiltype communicating to a geotechnician 
- If a good model is found, do VETO have plans/would want this to have a continuos learning capability. ie by adding labels of pre labelled data.

- Geological domain knowledge from survey data , homogeneous signal/heterogeneous signal. Sharp boundarys vs smooth transitions
- Segmentation, multi instance learning 
- hidden markov model
- [Introduction to Cone Penetration Testing](https://www.ags.org.uk/2022/09/introduction-to-cone-penetration-testing/)
- [Robertson](https://www.cpt-robertson.com/PublicationsPDF/Robertson%20Updated%20SBT%20CGJ%202016.pdf)
- first define soil types using known cone penetration metrics then see which soil lithostart correspond to this then do the prediction


### Belgium-specific CPT/Lithostrat References


* **Rogiers et al. 2017 (PLOS ONE)** — shows **automated lithostrat mapping from CPT SBT** on a large dataset in **northern Belgium**; compares SBT-based modeling vs unsupervised/literature charts and reports efficiency + accuracy gains. ([PLOS](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0176656))
* **Deckers & Goolaerts 2022 (Geologica Belgica)** — **CPT characterization of Middle–Upper Miocene** near **Antwerp Intl. Airport** (Berchem Fm: **Kiel & Antwerpen mbrs**); documents typical qc/Rf expressions and regional correlations. ([popups.uliege.be](https://popups.uliege.be/1374-8505/index.php?id=6999))
* **Schiltz 2020 (Geologica Belgica)** — **NE Belgium (≈60 km²)** case; proposes an **informal stratigraphy from CPT signals** and discusses which CPT-derived patterns distinguish units. ([popups.uliege.be](https://popups.uliege.be/1374-8505/index.php?id=6668&lang=en))
* **DOV Vlaanderen – Sonderingen** — official Flemish **definitions & measurement details** (qc, fs, **Rf**, u₂ optional; spacing, acquisition); aligns your variables with the **DOV data model**. ([Databank Ondergrond Vlaanderen](https://www.dov.vlaanderen.be/page/sonderingen))
* **NCS 2022 Excursion Guide (Neogene of N Belgium)** — regional context; notes **recent improvements in CPT-based correlations** for Antwerp-area units; useful when mapping CPT patterns to named formations. ([Databank Ondergrond Vlaanderen](https://dov.vlaanderen.be/sites/default/files/pfiles_files/NCS2022_ExcursionGuide.pdf))
* **Robertson CPT guides (SBT/qtn–Fr)** — the **standard charts/updates** underpinning SBT interpretation used in Belgian work; keep handy for **qₙ, qc, Rf** mapping. ([CPT Robertson](https://www.cpt-robertson.com/PublicationsPDF/2-56%20RobSBT.pdf))
* **pydov (DOV client) notebooks** — practical examples to **query CPTs by bbox/formation**, fetch profiles, and assemble **median qc/Rf** by depth for template building. ([Pydov](https://pydov.readthedocs.io/en/stable/notebooks/search_sonderingen.html))

- data management plan
- not all information to start implementing
- relation database; tables
- Preprocessing steps, cleaning, filtering, feature engineering: Talk this to the client; works equally good
- minimal viable product (MVP)
- derivative features, interactions, kNN, PCA, tsne, likelihood of layers, then which specific layers
- Sliding window approach, Modelling approach, 80% likelihood of this. Depth, geographical location, spatial correlation, depth profile,
- sequence, temporal patterns,
- multi instance learning, transition period, transient periods
- predict transition periods
- ensemble models, xgboost, lightgbm, catboost
- missing layers - imputation, knn imputation, mice imputation
- learn characteristics of missing layers
- markov models??
- look at the sequence of layer transitions
- geological domain knowledge, homogeneous signal/heterogeneous signal. Sharp boundaries vs smooth transitions
- first maybe group similar layers together, then predict the groups, then within the groups predict the specific layers
- handle removed layers using geotechnical information on sequentiality

```{r}
library(tidyverse)
library(data.table)
library(arrow)
library(here)
library(xgboost)
library(knitr)
library(zoo)
main_folder <- "year2/Project_Data_Science/project"
data_folder <- here(main_folder, "data")
results_folder <- here(main_folder, "results")
cpt_df <- read_parquet(
  paste(
    data_folder,
    "vw_cpt_brussels_params_completeset_20250318_remapped.parquet",
    sep = "/"
  )
)
setDT(cpt_df)
```


- head

```{r}
# filter of na in lithostrat_id
lithostrat_missing_df <- cpt_df[is.na(lithostrat_id)]
cpt_df <- cpt_df[!is.na(lithostrat_id)]
head(cpt_df) %>% kable()
```

```{r}

# read file from data_folder
lithostrat_label_order <- fread(
  paste(
    data_folder,
    "Copy of lithostrat_label_order - lithostrat_label_order.csv",
    sep = "/"
  )
)
kable(lithostrat_label_order)
```




```{r}
soldering_id_with_litho_missing <- unique(lithostrat_missing_df$sondering_id)
# without lithostrat_id missing
soldering_id_with_litho <- unique(cpt_df$sondering_id)

id_to_check <- 493
common_soldering_id <- intersect(soldering_id_with_litho_missing, soldering_id_with_litho)
soldering_315_df <- cpt_df[sondering_id == id_to_check]
soldering_315_df_without_litho <- lithostrat_missing_df[sondering_id == id_to_check]

```

```{r}
cpt_df[, .N, by = sondering_id][order(-N)]
cpt_df[, .(unique_drillings = uniqueN(sondering_id))]
```

```{r}
# number of obs per soldering id
cpt_df[, no_obs := .N, by = sondering_id]
obs <- cpt_df[, .(
  min_obs = min(no_obs),
  max_obs = max(no_obs),
  mean_obs = mean(no_obs),
  median_obs = median(no_obs)
)]

kable(obs)
```


- find unique drillings  then map

```{r, fig.width=10, fig.height=6}
unique_drillings <- cpt_df |>
  unique(by = "sondering_id")

## convert lambert to plot to plot with geom_sf
library(sf)
library(ggspatial)
library(ggrepel)


unique_drillings[, lithostrat_id_aval := ifelse(is.na(lithostrat_id), "missing", "available")]
unique_drillings_sf <- st_as_sf(unique_drillings,
  coords = c("x", "y"), crs = 31370
) |>
  st_transform(crs = 4326)

head(unique_drillings_sf) %>% kable()

# be_lvl2 <- geodata::gadm(
#   country = "BEL",
#   level = 2, path = tempdir()
# ) |>
#   st_as_sf() %>%
#    filter(NAME_2 %in% c("Vlaams Brabant", "Oost-Vlaanderen"))
# 
# save(be_lvl2, file = paste0(data_folder, "/be_lvl2.rda"))
load(paste0(data_folder, "/be_lvl2.rda"))
ggplot(be_lvl2) +
  geom_sf(fill = "grey96") +
  geom_sf(
    data = unique_drillings_sf,
    aes(geometry = geometry, color = lithostrat_id_aval),
     size = 0.5
  ) +
  geom_sf_text(
    aes(geometry = geometry,label = NAME_2)
  ) +
  labs(title = "CPT Drillings in Belgium Region") +
  theme_minimal()+
  theme(legend.position = "bottom")
```

- unique solderingID by lithostrat_id

```{r}
soldering_id_by_litho <- unique(cpt_df, 
                                by = c("sondering_id", 
                                       "lithostrat_id"))
```

- sequence of lithostrat_id per soldering id
- why important:understand layer sequences

```{r}
cpt_seq <- cpt_df[, .(col_seq = paste0(rev(unique(lithostrat_id)), 
                                       collapse = "|"),
                      len = uniqueN(lithostrat_id), 
                  layers = list(rev(unique(lithostrat_id)))),
                  by = sondering_id]

# depth by soldering id
soldering_depth <- cpt_df[, .(min_depth = min(diepte, na.rm = TRUE),
                           max_depth = max(diepte, na.rm = TRUE),
                           obs = .N),
                       by = sondering_id]
soldering_depth[, depth_range := max_depth - min_depth]

# merge
cpt_seq <- merge(cpt_seq, 
                 soldering_depth, 
                 by = "sondering_id", 
                 all.x = TRUE)
# filter off where len > 1
#cpt_seq <- cpt_seq[len > 1]
head(cpt_seq, 20) %>% kable()
```


```{r}

# Extract all consecutive pairs per sondering
pair_dt <- cpt_seq[, {
  lyr <- unlist(layers)
  if (length(lyr) < 2) {
    data.table(from_layer = character(), to_layer = character())
  } else {
    data.table(
      from_layer = lyr[-length(lyr)],
      to_layer   = lyr[-1]
    )
  }
}, by = sondering_id]

pair_dt[, two_layers := paste(from_layer, to_layer, sep = " -> ")]
kable(pair_dt[1:10])
```


```{r}
#melt to get individual lirthostrat_id

pair_dt_melt <- melt(pair_dt,
                     id.vars = c("sondering_id", "two_layers"),
                     measure.vars = c("from_layer", "to_layer"),
                     variable.name = "layer_type",
                     value.name = "lithostrat_id")

soldering_depth_li <- cpt_df[, .(min_depth = min(diepte, na.rm = TRUE),
                           max_depth = max(diepte, na.rm = TRUE),
                           obs = .N),
                       by = c("sondering_id", "lithostrat_id")]
# calculate expected observations per depth range
soldering_depth_li[, expected_obs := ceiling((max_depth - min_depth) / 0.02)]  # assuming 2 cm intervals
# merge with pair_dt_melt
pair_dt_melt <- merge(pair_dt_melt, 
soldering_depth_li, by = c("sondering_id", "lithostrat_id"),
 all.x = TRUE)
head(pair_dt_melt, 20) %>% kable()

```

### Soil Behavior Type Classification based on Robertson (2016)


-  rf bands : 


### Core SBT charts / banding (qc, Rf, qtn, fr, Ic)

* [Robertson (2016) — CPT-based SBT classification system (update, CGJ)](https://www.cpt-robertson.com/PublicationsPDF/Robertson%20Updated%20SBT%20CGJ%202016.pdf)
* [Robertson — CPT Guide (7th ed., concise handbook)](https://www.cpt-robertson.com/PublicationsPDF/CPT-Guide-7th-Final-SMALL.pdf)
* [Lunne, Robertson & Powell (1997/2002) — *Cone Penetration Testing in Geotechnical Practice* (CRC Press page)](https://www.taylorfrancis.com/books/mono/10.1201/9781482295047/cone-penetration-testing-geotechnical-practice-lunne-powell-robertson)

### Standards / definitions (how rf is defined, instrumentation & QA/QC)

* [ISO 22476-1:2022 — Electrical cone & piezocone penetration test (ISO page)](https://www.iso.org/standard/75661.html)
* [ASTM D5778 — Electronic friction cone & piezocone penetration test (ASTM page)](https://www.astm.org/d5778-20.html)

### Belgium-focused validations (linking CPT bands to lithostratigraphy)

* [Rogiers et al. (2017) — Model-based CPT classification & automated lithostrat mapping (PLOS ONE)](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0176656)
* [Schiltz (2020) — Using CPTs in stratigraphy; derived parameters vs qc/Rf (Geologica Belgica)](https://popups.uliege.be/1374-8505/index.php?id=6668&lang=en)
* [Deckers & Everaert (2022) — CPT characterization of Miocene units near Antwerp (Geologica Belgica PDF)](https://popups.uliege.be/1374-8505/index.php?file=1&id=7003&pid=6999)

### Practitioner intros / context

* [AGS (2022) — Introduction to Cone Penetration Testing](https://www.ags.org.uk/2022/09/introduction-to-cone-penetration-testing/)
* [pydov — Example notebooks for querying DOV CPT data](https://pydov.readthedocs.io/en/stable/notebooks/search_sonderingen.html)



```{r}

cpt_df[, rf_band := fcase(
  !is.na(rf) & rf < 1.0,                    "<1%",
  !is.na(rf) & rf < 1.5,                    "1–1.5%",
  !is.na(rf) & rf < 3.0,                    "1.5–3%",
  !is.na(rf) & rf < 5.0,                    "3–5%",
  !is.na(rf) & rf < 100,                    "5–100%",
  is.na(rf),                                     NA_character_,
  default = ">=100%" 
)]
cpt_df[, rf_band := factor(rf_band,
  levels = c("<1%","1–1.5%","1.5–3%","3–5%","5–100%",">=100%"),
   ordered = TRUE)]

```

- qc band

```{r}
cpt_df[, qc_band := fcase(
  !is.na(qc) & qc <  2,   "<2",
  !is.na(qc) & qc <  4,   "2–4",
  !is.na(qc) & qc < 10,   "4–10",
  !is.na(qc) & qc < 15,   "10–15",
  !is.na(qc) & qc < 30,   "15–30",
  !is.na(qc) & qc >= 30,  ">=30",
  default = NA_character_
)]
cpt_df[, qc_band := factor(qc_band,
  levels = c("<2","2–4","4–10","10–15","15–30",">=30"), 
  ordered = TRUE)]

```



```{r}
cpt_df[, fr_band := fcase(
  !is.na(fr) & fr < 0.5,         "<0.5%",
  !is.na(fr) & fr < 1.0,         "0.5–1%",
  !is.na(fr) & fr < 2.0,         "1–2%",
  !is.na(fr) & fr < 4.0,         "2–4%",
  !is.na(fr) & fr < 7.0,         "4–7%",
  !is.na(fr) & fr < 15.0,        "7–15%",
  !is.na(fr) & fr >= 15.0,       ">=15%",
  default = NA_character_
)]
cpt_df[, fr_band := factor(fr_band,
  levels = c("<0.5%","0.5–1%","1–2%",
             "2–4%","4–7%","7–15%",">=15%"), 
  ordered = TRUE)]
```


```{r}
cpt_df[, qtn_band := fcase(
  !is.na(qtn) & qtn < 10,        "<10",
  !is.na(qtn) & qtn < 20,        "10–20",
  !is.na(qtn) & qtn < 50,        "20–50",
  !is.na(qtn) & qtn < 100,       "50–100",
  !is.na(qtn) & qtn < 200,       "100–200",
  !is.na(qtn) & qtn >= 200,      ">=200",
  default = NA_character_
)]
cpt_df[, qtn_band := factor(qtn_band,
  levels = c("<10","10–20","20–50","50–100","100–200",">=200"), ordered = TRUE)]

```


```{r, fig.width=10, fig.height=6}
plot_litho_stack <- function(data,
                             litho_col = "lithostrat_id",
                             class_col = "behavior_basic",
                             title = NULL,
                             palette = NULL) {
  stopifnot(all(c(litho_col, class_col) %in% names(data)))

  df <- data[, .N, by = c(litho_col, class_col)]
  df[, prop := N / sum(N), by = litho_col]

  if (is.null(title)) {
    title <- sprintf("Share of %s classes within each %s",
                     class_col, litho_col)
  }

  ggplot(df, aes(x = .data[[litho_col]], y = prop, fill = .data[[class_col]])) +
    geom_col(position = "stack", width = 0.9) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_manual(values = palette %||% scales::hue_pal()(n_distinct(df[[class_col]]))) +
    labs(
      x = "Lithostratigraphic unit",
      y = "Share (%)",
      fill = class_col,
      title = title
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 35, hjust = 1))
}



```


```{r, fig.width=10, fig.height=6}
band_nms <- cpt_df[,.SD, .SDcols = patterns("band$")] %>%
  names()
for (bnm in band_nms) {
  p = plot_litho_stack(cpt_df, class_col = bnm,
                   title = paste("Distribution of", bnm, "by lithostrat_id")) 
  print(p)
}


```







### What we learned from the literature (CPT soil subdivision)

* From the literature, we learned that **soil layers can be divided using CPT-based soil properties** such as cone resistance (**qc**), sleeve friction (**fs**), friction ratio (**Rf = fs/qc**), and normalized indices (**Qtn**, **Fr**).
* **Standard CPT charts** (e.g., Robertson) link these properties to **soil behavior types (SBT)** like clay, silt, sand, and dense sand/gravel.
* Several Belgian studies have shown how CPT data can be used to **map lithostratigraphic units more efficiently**:


### Figure: CPT Robertson plots

* The figure below is **derived from CPT measurements**, guided by **methods described in the literature**.
* It shows how **different soil types can be distinguished** using measured and normalized CPT properties:

  * Left: **qc–Rf space**  shows the relation between cone resistance and friction ratio.
  * Right: **Qtn–Fr space** shows normalized values useful for comparing different sites.
* Colors represent **soil behavior groups** such as *silt/sandy silt*, *sand*, *mixed/transitional*, and *dense sand/gravel*.




```{r, fig.width=10, fig.height=6}

library(scales)
library(patchwork)
library(hexbin)


cpt_df[, rf_ratio := fs / qc]
cpt_df[, rf_pct   := 100 * rf_ratio]

# 'fr' appears to be percent already but can have big outliers.
# Make an explicit 'fr_pct' and lightly winsorize for visuals/features.
cpt_df[, fr_pct := as.numeric(fr)]
cpt_df[is.finite(fr_pct), fr_pct := pmin(fr_pct, 20)]   


# summary(cpt_df[, .(qc, fs, rf_ratio, rf_pct, qtn, fr_pct)])


# Prefer normalized Qtn–Fr space if both present; otherwise fall back to qc–Rf.
has_norm <- all(c("qtn", "fr_pct") %in% names(cpt_df))

if (has_norm) {
  # thresholds are sensible starters; tune per basin if desired
  cpt_df[, behavior := fcase(
    fr_pct >= 2   & qtn < 10,                     "Clay / organic",
    fr_pct >= 1   & qtn >= 10 & qtn < 50,         "Silt / sandy silt",
    fr_pct <  1.5 & qtn >= 50 & qtn < 200,        "Sand",
    fr_pct <  0.5 & qtn >= 200,                   "Dense sand / gravel",
    default = "Mixed / transitional"
  )]
} else {
  cpt_df[, behavior := fcase(
    qc < 0.5  & rf_pct > 5,                       "Organic / peat",
    qc < 1.5  & rf_pct >= 2,                       "Clay (soft)",
    qc < 4.0  & rf_pct >= 1.5,                     "Clay / silty clay",
    qc >= 4.0  & qc < 10  & rf_pct <= 1.5,         "Sand (loose)",
    qc >= 10.0 & qc < 20  & rf_pct <= 1.2,         "Sand (medium)",
    qc >= 20.0 & qc < 40  & rf_pct <= 1.0,         "Sand (dense)",
    qc >= 40.0 & rf_pct <  0.5,                    "Dense sand / gravel",
    default = "Mixed / transitional"
  )]
}

cpt_df[, behavior := factor(behavior)]


```


```{r}
plot_robertson_pair <- function(dt,
                                sonder_id,
                                color_var = "behavior",
                                palette = "Set2",
                                point_alpha = 0.55,
                                point_size = 1,
                                title_prefix = "CPT Robertson plots") {
  stopifnot(data.table::is.data.table(dt))
  stopifnot(color_var %in% names(dt))

  sub_dt <- dt[sondering_id == sonder_id]
  if (nrow(sub_dt) == 0L) stop("No rows for sondering_id = ", sonder_id)

  col_sym <- rlang::ensym(color_var)

  pA <- ggplot(sub_dt, aes(x = rf_pct, y = qc, color = !!col_sym)) +
    geom_point(alpha = point_alpha, size = point_size) +
    scale_y_log10(name = expression(paste("Cone resistance  ", q[c], " (MPa)")),
                  breaks = c(0.1, 0.5, 1, 5, 10, 50),
                  limits = c(0.1, 60)) +
    scale_x_continuous(name = expression(paste("Friction ratio  ", R[f], " (%)")),
                       limits = c(0, 8),
                       expand = expansion(mult = c(0.01, 0.05))) +
    labs(title = "qc–Rf space", color = color_var) +
    theme_minimal(base_size = 11)

  pB <- ggplot(sub_dt, aes(x = fr_pct, y = qtn, color = !!col_sym)) +
    geom_point(alpha = point_alpha, size = point_size) +
    scale_x_log10(name = expression(paste("Normalized friction ratio  ", F[r], " (%)")),
                  breaks = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
                  limits = c(0.1, 10)) +
    scale_y_log10(name = expression(paste("Normalized cone resistance  ", Q[t][n])),
                  breaks = c(1, 5, 10, 50, 100, 500),
                  limits = c(1, 600)) +
    labs(title = "Qtn–Fr space", color = color_var) +
    theme_minimal(base_size = 11)

  if (!is.null(palette)) {
    pA <- pA + scale_color_brewer(palette = palette)
    pB <- pB + scale_color_brewer(palette = palette)
  }

  (pA | pB) +
    patchwork::plot_annotation(
      title = sprintf("%s — sondering %s", title_prefix, sonder_id)
    )
}

set.seed(300)
ids <- sample(cpt_df[!is.na(lithostrat_id), unique(sondering_id)], 10)
for (sid in ids) print(plot_robertson_pair(cpt_df, sid, color_var = "behavior"))

```


- Color the same abobe with  lithorstat ID

```{r}
for (sid in ids) print(plot_robertson_pair(cpt_df[!is.na(lithostrat_id)], 
                                           sid, color_var = "lithostrat_id"))
```


- Randomly select 10 drillings and plot cpt data

```{r, fig.width=10, fig.height=6}
set.seed(124)
unique_sonde_lithor <- cpt_df |>
  unique(by = c("sondering_id", "lithostrat_id"))

unique_sonde_n <- unique_sonde_lithor[, .(freq = .N), by = sondering_id][freq>=2]
sampled_drillings <-  unique_sonde_n[, sample(unique(sondering_id), 10)]
sampled_data <- cpt_df[sondering_id %in% sampled_drillings]
data.table::setorder(sampled_data, index)



plot_cpt_series <- function(data,
                            depth_col = "diepte",
                            value_col = "qc",
                            color_var = "lithostrat_id",
                            group_var = NULL,
                            facet_var = "sondering_id",
                            title = NULL,
                            depth_label = "Depth below surface (m)",
                            value_label = NULL,
                            ncol_facet = 3,
                            log_value = FALSE,
                            reverse_depth = TRUE,
                            flip_coords = TRUE,
                            alpha_line = 0.7,
                            alpha_point = 0.6,
                            point_size = 0.6,
                            legend_position = "bottom") {
  depth_sym <- rlang::ensym(depth_col)
  value_sym <- rlang::ensym(value_col)
  color_sym <- if (is.null(color_var)) {
    NULL
  } else {
    rlang::ensym(color_var)
  }
  group_sym <- if (is.null(group_var) && !is.null(color_var)) {
    rlang::ensym(color_var)
  } else if (is.null(group_var)) {
    rlang::ensym(facet_var)
  } else {
    rlang::ensym(group_var)
  }
  value_label <- value_label %||% value_col

  p <- ggplot(data, aes(x = !!depth_sym, y = !!value_sym)) +
    geom_line(aes(group = interaction(!!rlang::enquo(group_sym))), alpha = alpha_line) +
    geom_point(aes(group = interaction(!!rlang::enquo(group_sym))), size = point_size, alpha = alpha_point)

  if (!is.null(color_sym)) {
    p <- p + aes(color = factor(!!color_sym))
  }

  if (log_value) {
    p <- p + scale_y_log10()
  }
  if (reverse_depth) {
    p <- p + scale_x_reverse()
  }
  if (flip_coords) {
    p <- p + coord_flip()
  }

  p +
    facet_wrap(stats::as.formula(paste("~", facet_var)), ncol = ncol_facet) +
    labs(
      title = title %||% sprintf("CPT Profiles (%s vs depth)", value_col),
      x = depth_label,
      y = value_label,
      color = color_var %||% NULL
    ) +
    theme_minimal() +
    theme(legend.position = legend_position)
}


plot_cpt_series(sampled_data,
  value_col = "qc",
  title = "Cone resistance"
)
```

```{r, fig.width=10, fig.height=10}
plot_cpt_series(sampled_data,
  value_col = "fr",
  title = "friction"
)
```

- lets smooth the qc and fs with rolling mean of window 3
- is the frequency obeying some symmetry

```{r}
# use decompose function to
extract_trend <- function(x, freq = 5L) {
  x <- as.numeric(x)
  # If frequency invalid or not enough data for 2 periods, return original series
  if (!is.finite(freq) || freq < 2L) return(x)
  if (length(na.omit(x)) < 2L) return(x)
  if (length(x) < 2L * freq) return(x)

  out <- tryCatch({
    dc <- decompose(ts(x, frequency = freq))
    tr <- dc$trend
    # If decompose produced all-NA trend, fall back to original; otherwise fill edge NAs with original values
    if (all(is.na(tr))) x else ifelse(is.na(tr), x, tr)
  }, error = function(e) x, warning = function(w) x)

  out
}

freq_from_depth <- function(z, default = 25L) {
  if (length(z) < 2L) {
    return(default)
  }
  dz <- stats::median(diff(sort(z)), na.rm = TRUE)
  if (!is.finite(dz) || dz <= 0) {
    return(default)
  }
  as.integer(max(1, round(1 / dz)))
}



sampled_data[, fr_smooth := extract_trend(fr, freq = freq_from_depth(diepte)),
  by = sondering_id
]
sampled_data[, qc_smooth := extract_trend(qc, freq = freq_from_depth(diepte)),
  by = sondering_id
]
```


- plot qc_smooth

```{r , fig.width=10, fig.height=6}
plot_cpt_series(sampled_data[!is.na(lithostrat_id)],
  value_col = "qc_smooth",
  title = "Smoothed Cone resistance (trend)"
)
```

- plot fr_smooth

```{r}
plot_cpt_series(sampled_data,
  value_col = "fr_smooth",
  title = "Smoothed friction (trend)"
)
```

```{r}
cpt_df_all <- copy(cpt_df)
cpt_df <- cpt_df[!is.na(lithostrat_id)]
```


```{r}
library(signal)
library(pracma)
# Zero-phase Butterworth low-pass
butter_zero_phase <- function(x, dz, fc = 1.0, order = 4L) {
  # fc in cycles/m; dz in meters/sample
  if (!is.finite(dz) || dz <= 0 || all(!is.finite(x))) return(x)
  nyq <- 1/(2*dz)                    # Nyquist (cycles/m)
  Wc  <- min(0.99, max(1e-6, fc/nyq))# normalized cutoff 0..1
  bf  <- signal::butter(order, Wc, type = "low")
  # detrend first to keep DC drift from dominating:
  xd  <- pracma::detrend(x, tt = "linear")
  as.numeric(signal::filtfilt(bf, xd))
}

vars_to_plot <- c("qc", "fs", "rf", "qtn", "fr")

filterd_vars <- paste0(vars_to_plot, "butter_zero")
cpt_df[, dz := median(diff(diepte), na.rm = TRUE), by = sondering_id]
cpt_df[, (filterd_vars) := lapply(.SD, butter_zero_phase, dz =dz[1]),
       .SDcols = vars_to_plot,
       by = sondering_id
]

```


 
```{r}
# boxplotqc, rf_pct, qtn, fr by lithostrat_id

vars_to_plot <- c("qc", "fs", "rf", "qtn",  "fr")

lapply(X = vars_to_plot, FUN = function(var) {
    p <- ggplot(cpt_df, aes(x = lithostrat_id, y = log(.data[[var]]), fill = lithostrat_id)) +
        geom_boxplot() +
        scale_y_log10() +
        labs(
        title = paste("Distribution of", var, "Values by Lithostrat Unit"),
        x = "Lithostratigraphic Unit",
        y = paste(var, "[log scale]"),
        fill = "Lithostrat unit"
        ) +
        theme_minimal() +
        theme(
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none"
        )
    p
    
    })


```



- plot series of filtered vars 

```{r, eval=FALSE}
for(var in filterd_vars) {
  p <- plot_cpt_series(cpt_df[sondering_id %in% ids],
    value_col = var,
    title = paste("Filtered", var, "vs Depth")
  )
  print(p)
}
```


```{r qc_summaries_simple, fig.width=12, fig.height=6}
cpt_model_df <- cpt_df[!is.na(rf)]

# 1) Five-number summary of qc PER DRILLING (sondering_id)

# cut depth into bins of 1m
cpt_model_df[, diepte := round(diepte)]
cpt_model_df[, diepte_bin := cut(diepte, breaks = seq(
  from = 0, to = max(diepte, na.rm = TRUE), by = 5
))]

# remove N
```









- bar plot of lithostrat_id  vs depth bins

```{r litho_depth_bar, fig.width=10, fig.height=6}
# bar plot of lithostrat_id  vs depth bins
ggplot(cpt_model_df, aes(x = diepte_bin, fill = as.factor(lithostrat_id))) +
  geom_bar(position = "fill") +
  labs(
    title = "Proportion of Lithostrat Units by Depth Bins",
    x = "Depth Bins (m)",
    y = "Proportion",
    fill = "Lithostrat unit"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )
```


```{r}
value_vars <-c("qc", "fs", "rf", "qtn", "fr")
diff_vars <- paste0(value_vars, "_diff") 
# calculate diff by soldering id 
cpt_model_df[, (diff_vars) := lapply(.SD, function(x) c(NA, diff(x))),
              .SDcols = value_vars,
              by = sondering_id
              ]

```

```{r}
trend_vars <- paste0(value_vars, "_trend")
# calculate trend by soldering id
cpt_model_df[, (trend_vars) := lapply(.SD, function(x) extract_trend(x, freq = freq_from_depth(diepte))),
              .SDcols = value_vars,
              by = sondering_id
              ]
              

```


```{r}


dcast_cpt_wide <- function(dt,
                           value_cols,
                           depth_bin = "diepte_bin",
                           id_cols = c("sondering_id", "lithostrat_id"),
                           agg_funs = list(
                                           median =  function(x) median(x, na.rm = TRUE),
                                           mean = function(x) mean(x, na.rm = TRUE),
                                           q25 = function(x) quantile(x, 0.25, na.rm = TRUE),
                                           q75 = function(x) quantile(x, 0.75, na.rm = TRUE),
                                           min = function(x)  min(x, na.rm = TRUE),
                                           max = function(x)  max(x, na.rm = TRUE)),
                           fill_value = 0,
                           na_rm_cols = TRUE) {
  stopifnot(all(value_cols %in% names(dt)))
  
  if (is.function(agg_funs)) {
    agg_funs <- list(custom = agg_funs)
  }
  if (is.null(names(agg_funs))) {
    names(agg_funs) <- paste0("fn", seq_along(agg_funs))
  }
  
  wide_list <- lapply(value_cols, function(vcol) {
    lapply(seq_along(agg_funs), function(i) {
      fun <- agg_funs[[i]]
      fun_nm <- names(agg_funs)[i]
      cat(fun_nm)
      out <- data.table::dcast(
        formula = as.formula(paste(
          paste(id_cols, collapse = "+"), "~", depth_bin
        )),
        value.var = vcol,
        fun.aggregate = fun,
        data = dt,
        fill = fill_value
      )
      
      data_cols <- setdiff(names(out), id_cols)
      setnames(out, data_cols, paste0(data_cols, "_", vcol, "_", fun_nm))
      out
    })
  }) |>
    unlist(recursive = FALSE)
  
  wide_dt <- Reduce(function(a, b)
    merge(a, b, by = id_cols, all = TRUE), wide_list)
  
  if (na_rm_cols) {
    keep <- colSums(is.na(wide_dt)) < nrow(wide_dt)
    wide_dt <- wide_dt[, ..keep]
  }
  
  wide_dt[]
}


# bins of one meter
cpt_model_df <- cpt_model_df[!is.na(diepte)]
cpt_model_df[, diepte_bin := cut(diepte,
  breaks = seq(
    from = 0, to = max(diepte, na.rm = TRUE) + 5, by = .6
  ),
  include.lowest = TRUE
)]


# cpt_model_df[!is.na(fs), fs_smooth := extract_trend(fs, freq = freq_from_depth(diepte)),
#              by = sondering_id]
# cpt_model_df[!is.na(qc), qc_smooth := extract_trend(qc, freq = freq_from_depth(diepte)),
#              by = sondering_id]


value_vars <-c("qc", "fs", "rf", "qtn", "fr")


cpt_wide <- dcast_cpt_wide(cpt_model_df,
  value_cols = c(trend_vars),
  depth_bin = "diepte_bin",
  id_cols = c("sondering_id", "lithostrat_id"),
  agg_funs = list(
    median = function(x) median(x, na.rm = TRUE),
    mean = function(x) mean(x, na.rm = TRUE),
    q25 = function(x) quantile(x, 0.25, na.rm = TRUE),
    q75 = function(x) quantile(x, 0.75, na.rm = TRUE),
    min = function(x) min(x, na.rm = TRUE),
    max = function(x) max(x, na.rm = TRUE)
  ),
  fill_value = 0,
  na_rm_cols = TRUE
)

#cpt_wide <- cpt_wide[sondering_id %in% pair_dt$sondering_id]
```


```{r}


widen_factor_indicators <- function(dt,
                                    factor_cols,
                                    id_cols = c("sondering_id", "lithostrat_id"),
                                    na_label = NULL) {
  stopifnot(data.table::is.data.table(dt))
  stopifnot(all(c(factor_cols, id_cols) %in% names(dt)))

  widen_one <- function(col) {
    tmp <- dt[, c(id_cols, col), with = FALSE]
    setnames(tmp, col, "factor_value")
    if (!is.null(na_label)) tmp[is.na(factor_value), factor_value := na_label]

    wide <- dcast(
      tmp,
      as.formula(paste(paste(id_cols, collapse = " + "), "~ factor_value")),
      value.var = "factor_value",
      fun.aggregate = function(x) as.integer(any(!is.na(x))),
      fill = 0
    )

    value_cols <- setdiff(names(wide), id_cols)
    setnames(wide, value_cols, paste0(col, "_", value_cols))
    wide
  }

  merged <- Reduce(function(a, b) merge(a, b, by = id_cols, all = TRUE),
                   lapply(factor_cols, widen_one))
  num_cols <- setdiff(names(merged), id_cols)
  merged[, (num_cols) := lapply(.SD, function(v) fifelse(is.na(v), 0L, v)), .SDcols = num_cols]
  merged[]
}



factor_vars <- c("behavior", "rf_band", 
                 "qc_band", "fr_band", 
                 "qtn_band")

cpt_wide_factors <- widen_factor_indicators(cpt_model_df, factor_vars)

cpt_wide <- merge(cpt_wide, cpt_wide_factors,
  by = c("sondering_id", "lithostrat_id"),
  all.x = TRUE
)
```



```{r}
ggplot(cpt_wide, aes(x = lithostrat_id,
                     fill = as.factor(lithostrat_id))) +
  geom_bar() +
  labs(
    title = "Bar Plot of Lithostratigraphic Units",
    x = "Lithostratigraphic Unit",
    y = "Count",
    fill = "Lithostratigraphic Unit"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```


```{r}
# get freq by lithostrat_id
litho_counts <- cpt_wide[, .N, by = lithostrat_id]
# get lithostrat_id with more than 10 counts
rare_litho <- litho_counts[N < 5, lithostrat_id]
cpt_wide <- cpt_wide[!lithostrat_id %in% rare_litho,]
```




```{r}

library(xgboost)
library(rsample) 
library(yardstick)

cpt_wide[, lithostrat_id_group := lithostrat_id ]

# 0) Keep only rows with a target
cpt_wide <- cpt_wide[!is.na(lithostrat_id_group)]
#save(cpt_wide, file = here(data_folder, "cpt_wide.rda"))
# 1) Define features (drop ids/target)
feat_cols <- setdiff(names(cpt_wide),
                     c("sondering_id", "lithostrat_id", 
                       "lithostrat_id_group", "two_layers"))


# feat_cols_quoted <- paste0("`", feat_cols, "`")
# options(expressions = 50000)
# model_formula <- as.formula(
#   paste("lithostrat_id ~", paste(feat_cols, collapse = " + "))
# )

```

```{r}
ycol_xcols <- c("lithostrat_id", feat_cols, "sondering_id")
cpt_model_df2 <- cpt_wide[, ..ycol_xcols]
```



```{r, fitting_xgb, eval=TRUE}
library(tidymodels)


# build weights
w_lookup <- cpt_model_df2 %>%
  count(lithostrat_id, name = "n") %>%
  mutate(w = max(n) / n)

cpt_w <- cpt_model_df2 %>%
  left_join(w_lookup, by = "lithostrat_id") %>%
  mutate(wts = hardhat::importance_weights(w)) %>%
  select(-w)
```


```{r}
group_strat_split <- function(dt,
                              id_col = "sondering_id",
                              y_col = "lithostrat_id",
                              prop = 0.7,
                              tol = 0.05,
                              max_tries = 200,
                              seed = 42) {
  setDT(dt)
  ids_lab <- dt[, .(mode_class = names(which.max(table(get(
    y_col
  ))))), by = id_col]
  full_counts <- dt[, .N, by = y_col]
  
  for (s in seq_len(max_tries)) {
    set.seed(seed + s - 1)
    spl <- initial_split(as.data.frame(ids_lab),
                         prop = prop,
                         strata = mode_class)
    tr_ids <- training(spl)[[id_col]]
    tr_dt  <- dt[get(id_col) %in% tr_ids]
    tr_counts <- tr_dt[, .N, by = y_col]
    chk <- tr_counts[full_counts, on = y_col][, pct := i.N / N][, all(abs(pct - prop) <= tol)]
    
    if (isTRUE(chk)) {
      te_ids <- testing(spl)[[id_col]]
      return(list(
        train_ids = tr_ids,
        test_ids = te_ids,
        seed = seed + s - 1
      ))
    }
  }
  # fallback (last attempt)
  list(
    train_ids = tr_ids,
    test_ids = testing(spl)[[id_col]],
    seed = seed + max_tries - 1
  )
}

res <- group_strat_split(cpt_w, prop = 0.7, tol = 0.05)
train_dt <- cpt_w[sondering_id %in% res$train_ids]
test_dt  <- cpt_w[sondering_id %in% res$test_ids]

```




```{r}
# recipe: exclude wts from predictors in the formula (do not set case_weights role here)
library(RBF)
library(embed) 
xgb_rec <- recipe(lithostrat_id ~ . , data = train_dt |> select(-n)) |>
  update_role(sondering_id, new_role = "group_id") |>
  step_rm(sondering_id) |>
  step_zv(all_predictors()) |>
  step_nzv(all_predictors()) |>
  step_center(all_numeric_predictors()) |>
  step_impute_median(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors(), na_rm = TRUE) |>
  step_scale(all_numeric_predictors()) |>
  #step_corr(all_numeric_predictors(), threshold = 0.95) |>
  step_pca(all_numeric_predictors(), num_comp = 200) 
  #step_kpca(all_numeric_predictors(), num_comp = 50, kernel = "rbfdot", sigma = tune())

xgb_spec <- boost_tree(
  trees          = 500,
  tree_depth     = tune(),
  learn_rate     = tune(),
  loss_reduction = tune(),
  min_n          = tune(),
  mtry           = tune(),
  sample_size    = tune()
) |>
  set_mode("classification") |>
  set_engine("xgboost")

# Ranger Random Forest spec (supports case weights via workflow)
rf_spec <- rand_forest(
  trees = 500,
  mtry  = tune(),
  min_n = tune()
) |>
  set_mode("classification") |>
  set_engine("ranger", importance = "permutation")

xgb_wf <- workflow() |>
  add_recipe(xgb_rec) |>
  add_model(xgb_spec)

# Attach case weights to the workflow
xgb_wf_w <- xgb_wf |> add_case_weights(wts)

# RF workflow (reuse same recipe)
rf_wf <- workflow() |>
  add_recipe(xgb_rec) |>
  add_model(rf_spec)

# Attach case weights for RF
rf_wf_w <- rf_wf |> add_case_weights(wts)

```



```{r}
library(future)
library(furrr)

# grouped CV
set.seed(42)
folds <- group_vfold_cv(cpt_w, group = sondering_id, v = 10)

# params/grid
xgb_params <- extract_parameter_set_dials(xgb_wf) |>
  finalize(cpt_w |> dplyr::select(-lithostrat_id, -sondering_id, -wts, -n))

xgb_grid <- grid_latin_hypercube(xgb_params, size = 5)

# tune with weights (weights carried by the workflow)
set.seed(42)
plan(multisession, workers = parallel::detectCores() - 4)
xgb_res <- tune_grid(
  xgb_wf_w,
  resamples = folds,
  grid = xgb_grid,
  metrics =yardstick::metric_set(accuracy, mn_log_loss),
  control = control_grid(save_pred = TRUE)
)

rf_params <- extract_parameter_set_dials(rf_wf) |>
  finalize(cpt_w |> dplyr::select(-lithostrat_id, -sondering_id, -wts, -n))

rf_grid <- grid_latin_hypercube(rf_params, size = 5)

rf_res <- tune_grid(
  rf_wf_w,
  resamples = folds,
  grid = rf_grid,
  metrics = yardstick::metric_set(accuracy, mn_log_loss),
  control = control_grid(save_pred = TRUE)
)

plan(sequential) # back to sequential
# final fit (weights carried by the workflow)


```





#### xgb metrics

```{r}
xgb_metrics <- collect_metrics(xgb_res)
kable(xgb_metrics)

```

#### rf metrics

```{r}
rf_metrics <- collect_metrics(rf_res)
kable(rf_metrics)

```

```{r}
cmp <- bind_rows(
  xgb_metrics |> dplyr::mutate(model = "xgb"),
  rf_metrics  |> dplyr::mutate(model = "ranger")
)

cmp_summary <- cmp |>
  dplyr::group_by(model, .metric) |>
  dplyr::summarize(mean = mean(mean), std_err = mean(std_err), .groups = "drop") |>
  dplyr::arrange(.metric, dplyr::desc(mean))

knitr::kable(cmp_summary)

```



#### Final fit and predictions

```{r}

xgb_fit <- finalize_workflow(xgb_wf_w, select_best(xgb_res, metric = "mn_log_loss")) |>
  fit(data = train_dt)
xgb_preds_prob <- predict(xgb_fit, new_data = test_dt, type = "prob")
xgb_preds_class <- predict(xgb_fit, new_data = test_dt)



cpt_model_pred_xgb <- bind_cols(
  test_dt[, .(sondering_id, lithostrat_id)],
  xgb_preds_class,
  xgb_preds_prob
)

```

#### Metrics with Yardstick

```{r}
library(yardstick)

cpt_model_pred_xgb[, lithostrat_id := factor(lithostrat_id,
  levels = unique(cpt_wide$lithostrat_id)
)]
cpt_model_pred_xgb[, .pred_class := factor(.pred_class,
  levels = unique(cpt_wide$lithostrat_id)
)]
yardstick::conf_mat(
  data = cpt_model_pred_xgb,
  truth = lithostrat_id,
  estimate = .pred_class
)
```

```{r}
# xgb accuracy, auc, roc, balanced accuracy
metrics_set <- yardstick::metric_set(accuracy, bal_accuracy)

xgb_eval <- metrics_set(
  data = cpt_model_pred_xgb,
  truth = lithostrat_id,
  estimate = .pred_class
)
knitr::kable(xgb_eval)
```





```{r}
rf_fit <- finalize_workflow(rf_wf_w, select_best(rf_res, metric = "mn_log_loss")) |>
    fit(data = train_dt)
rf_preds_prob <- predict(rf_fit, new_data = test_dt, type = "prob")
rf_preds_class <- predict(rf_fit, new_data = test_dt)
cpt_model_pred_rf <- bind_cols(
    test_dt[, .(sondering_id, lithostrat_id)],
    rf_preds_class,
    rf_preds_prob
)
```

```{r}
cpt_model_pred_rf[, lithostrat_id := factor(lithostrat_id,
    levels = unique(cpt_wide$lithostrat_id)
)]
cpt_model_pred_rf[, .pred_class := factor(.pred_class,
    levels = unique(cpt_wide$lithostrat_id)
)]
conf_mat_xgb <- yardstick::conf_mat(
    data = cpt_model_pred_rf,
    truth = lithostrat_id,
    estimate = .pred_class
)

```



```{r}
# Long-format confusion with per-truth rates and per-class accuracy (diagonal) — data.table
cm_dt <- as.data.table(as.data.frame(conf_mat_xgb$table))
setnames(cm_dt, c("Prediction", "Truth", "Freq"),
               c("predicted_class", "lithostrat_id", "n"))

# Rate of each predicted class within each true lithostrat_id
cm_dt[, total := sum(n), by = .(lithostrat_id)]
cm_dt[, rate  := fifelse(total > 0, n/total, NA_real_)]

# Per-soil-type prediction accuracy (diagonal proportion)
per_class_accuracy_rf <- cm_dt[predicted_class == lithostrat_id,
  .(lithostrat_id, predicted_class, accuracy = rate, sample_size = total)
]

knitr::kable(per_class_accuracy_rf, caption = "Per-class prediction accuracy (RF)")
```
```


```{r}
# rf accuracy, auc, roc, balanced accuracy
metrics_set <- yardstick::metric_set(accuracy, bal_accuracy)

rf_eval <- metrics_set(
  data = cpt_model_pred_rf,
  truth = lithostrat_id,
  estimate = .pred_class
)
knitr::kable(rf_eval)
```

### Error Analaysis

#### Get misclassified drillings in XGB
```{r}
xgb_misclass <- cpt_model_pred_xgb[lithostrat_id != .pred_class]
fwrite(xgb_misclass, file = here(
    results_folder,
    "missclasified_cpt_model_predictions_xgb.csv"
))
knitr::kable(xgb_misclass)

```


```{r}
#  names(pair_dt_melt)
# [1] "sondering_id"  "lithostrat_id" "two_layers"    "layer_type"    "min_depth"     "max_depth"     "obs"           "expected_obs"
pairdt_missclass <- merge(
    xgb_misclass[, .(sondering_id, lithostrat_id, .pred_class)],
    pair_dt_melt,
    by = c("sondering_id", "lithostrat_id"),
    all.x = TRUE
)

fwrite(pairdt_missclass, file = here(
    results_folder,
    "missclasified_cpt_model_profiles_xgb.csv"
))
```


```{r, eval=FALSE}

library(keras3)


# ---- Inputs
dt <- as.data.table(cpt_model_df)

dt <- dt[!lithostrat_id %in% rare_litho]

```{r, eval=FALSE}

library(keras3)


# ---- Inputs 
dt <- as.data.table(cpt_model_df) 

dt <- dt[!lithostrat_id %in% rare_litho]
setorder(dt, sondering_id, diepte)

feat_cols <- c("qc", "fs", "rf", "qtn", "fr") # input features

# extract trend (group-wise frequency from depth; robust to short series)
dt[, fd := freq_from_depth(diepte), by = sondering_id]
dt[, (feat_cols) := lapply(.SD, function(v) extract_trend(v, freq = fd[1L])),
  .SDcols = feat_cols,
  by = sondering_id
]
dt[, fd := NULL]

label_col <- "lithostrat_id"   # per-depth factor label
id_col    <- "sondering_id"

# ---- Encode labels 0..K-1 (sparse) ----
dt[, (label_col) := as.factor(get(label_col))]
lab_levels <- levels(dt[[label_col]])
num_class  <- length(lab_levels)
# Reserve 0 for pads; real labels 1..K
dt[, y_id := as.integer(get(label_col))]   # 1..K integers
num_out <- num_class + 1L                  # output units include pad index 0

# ---- Train/valid split by sondering_id (keep groups intact) ----
set.seed(42)
ids   <- unique(dt[[id_col]])
tr_ids <- sample(ids, size = round(0.8 * length(ids)))
va_ids <- setdiff(ids, tr_ids)

train_dt <- dt[get(id_col) %in% tr_ids]
valid_dt <- dt[get(id_col) %in% va_ids]

# ---- Z-score scaling with train statistics ----
mu  <- train_dt[, lapply(.SD, mean, na.rm = TRUE), .SDcols = feat_cols]
sdv <- train_dt[, lapply(.SD, function(x) sd(x, na.rm = TRUE) + 1e-8), .SDcols = feat_cols]

train_scaled <- copy(train_dt)
valid_scaled <- copy(valid_dt)

# apply per-feature z-score using train means/sds
train_scaled[, (feat_cols) := lapply(feat_cols, function(nm) (get(nm) - mu[[nm]]) / sdv[[nm]])]
valid_scaled[, (feat_cols) := lapply(feat_cols, function(nm) (get(nm) - mu[[nm]]) / sdv[[nm]])]
setorderv(train_scaled, c(id_col, "diepte"))
setorderv(valid_scaled, c(id_col, "diepte"))

build_tensors <- function(dt, feat_cols, id_col, T_pad = NULL) {
  dt <- copy(dt)
  dt[, t_idx := seq_len(.N), by = id_col]
  if (is.null(T_pad)) {
    T_pad <- dt[, max(t_idx)]
  }

  id_levels <- dt[, unique(get(id_col))]
  dt[, id_factor := factor(get(id_col), levels = id_levels)]

  N <- length(id_levels)
  F <- length(feat_cols)

  feat_long <- melt(
    dt,
    id.vars = c("id_factor", "t_idx"),
    measure.vars = feat_cols,
    variable.name = "feature",
    variable.factor = FALSE,
    value.name = "value"
  )
  feat_long[, feature_idx := match(feature, feat_cols)]

  X <- array(0, dim = c(N, T_pad, F))
  X[cbind(
    as.integer(feat_long$id_factor),
    pmin(feat_long$t_idx, T_pad),
    feat_long$feature_idx
  )] <- feat_long$value

  index_core <- cbind(as.integer(dt$id_factor), pmin(dt$t_idx, T_pad))
  y <- array(0L, dim = c(N, T_pad))
  y[index_core] <- dt$y_id
  w <- array(0, dim = c(N, T_pad))
  w[index_core] <- 1

  list(X = X, y = y, w = w, T_max = T_pad, F = F, ids = id_levels)
}

train_lengths <- train_scaled[, .N, by = id_col]
valid_lengths <- valid_scaled[, .N, by = id_col]
T_pad <- max(c(train_lengths$N, valid_lengths$N))

train_arr <- build_tensors(train_scaled, feat_cols, id_col, T_pad)
valid_arr <- build_tensors(valid_scaled, feat_cols, id_col, T_pad)

X_tr <- train_arr$X
y_tr <- train_arr$y
w_tr <- train_arr$w

X_va <- valid_arr$X
y_va <- valid_arr$y
w_va <- valid_arr$w

T_max <- T_pad
F_in  <- length(feat_cols)

#Class-balanced sample weights (mask pads + upweight rare labels) ----
train_counts <- train_scaled[, .N, by = y_id]
# Build class weights for indices 0..K (0=pad placeholder)
cw <- rep(1, num_out)
cw[train_counts$y_id + 1L] <- max(train_counts$N) / train_counts$N
# normalize weights over real classes (ignore pad at index 1)
cw <- cw / mean(cw[-1L])
sw_tr <- cw[y_tr + 1L] * w_tr
sw_va <- cw[y_va + 1L] * w_va
# ensure numeric dtype for Keras
storage.mode(sw_tr) <- "double"
storage.mode(sw_va) <- "double"

# Define a compact, R-ish LSTM model 
build_model <- function(T_max, F_in, num_out,
                        units = 128L, dropout = 0.2, lr = 5e-4) {
  input <- layer_input(shape = c(T_max, F_in), name = "seq")
  output <- input |>
    layer_lstm(units = units, return_sequences = TRUE, name = "lstm") |>
    layer_dropout(rate = dropout, name = "drop") |>
    layer_dense(units = num_out, activation = "softmax", name = "out")

  model <- keras_model(input, output)
  model |> compile(
    optimizer = optimizer_adam(learning_rate = lr),
    loss      = "sparse_categorical_crossentropy",
    metrics   = "accuracy"
  )
  model
}

model <- build_model(T_max, F_in, num_out)

#Fit (masking via sample weights to ignore padding) ----
hist <- model |> fit(
  x = X_tr, y = y_tr,
  sample_weight = sw_tr,
  validation_data = list(X_va, y_va, sw_va),
  epochs = 80,
  batch_size = 64,
  callbacks = list(
    callback_early_stopping(monitor = "val_loss", ``patience = 8, restore_best_weights = TRUE),
    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.5, patience = 4, min_lr = 1e-5)
  )
)

## Sanity checks and evaluation
# Pads must align with zero weights
stopifnot(all((w_tr == 0) == (y_tr == 0)))
stopifnot(all((w_va == 0) == (y_va == 0)))

# Evaluate per-depth (flatten valid, drop pads; ignore predicted pad=0)
p_va <- model |> predict(X_va)
yhat_va <- apply(p_va, c(1,2), which.max) - 1L   # 0..K where 0=pad

mask <- as.vector(w_va) == 1
truth_idx <- as.vector(y_va)[mask]               # 1..K
pred_idx  <- as.vector(yhat_va)[mask]            # 0..K

# drop any predictions of pad (0) on real steps
keep <- pred_idx > 0L
truth_idx <- truth_idx[keep]
pred_idx  <- pred_idx[keep]

eval_dt <- data.table(
  truth = factor(truth_idx, levels = 1:num_class, labels = lab_levels),
  pred  = factor(pred_idx,  levels = 1:num_class, labels = lab_levels)
)

# quick pad fractions
pad_frac_tr <- mean(as.vector(w_tr) == 0)
pad_frac_va <- mean(as.vector(w_va) == 0)
print(c(pad_frac_tr = pad_frac_tr, pad_frac_va = pad_frac_va))


```

```{r, eval=FALSE}
metric_set <- yardstick::metric_set(accuracy,
                                    kap, f_meas, 
                                    recall, 
                                    precision)

kable(metric_set(eval_dt,
                 truth = truth, 
                 estimate = pred))
```

